{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Due date:_ \n",
    "\n",
    "_Part 1: October 6, 2025, 11:59 pm_\n",
    "\n",
    "_Part 2: October 13, 2025, 11:59 pm_\n",
    "\n",
    "_Please hand in your completed .ipynb file on Gradescope (you can work in the same notebook for part 2). Moreover, for part 2b, please hand in a separate pdf document. Make sure you name the file with your and your teammates' names, e.g. \"Anna_Amar_Mary_Guido_Assignment_1.ipynb\" and \"Anna_Amar_Mary_Guido_Policy_Memo_1.pdf\". Only one of you will need to hand in the file, but make sure you list everyone's name in the next cell._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>TO DO:</font>\n",
    "\n",
    "<font color='red'>Double-click on this box and fill in your names</font>\n",
    "\n",
    "**Names:**\n",
    "\n",
    "_Name 1_, _Name 2_, _Name 3_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In the first week of our experimental module, we discussed the difference between correlation and causality, noting that while correlation shows if variables move together, causality indicates if one variable causes a change in another. We explored how randomized control trials (RCTs) establish causality by creating treatment and control groups with similar characteristics, except for the treatment variable, which eliminates concerns about confounding variables.\n",
    "\n",
    "**What are we doing in this homework?**  \n",
    "\n",
    "In this HW, we will analyze a famous experiment in economics on an employment program, the National Supported Work (NSW) program. In part 1 of this homework, we will familiarize ourselves with the data by conducting an exploratory analysis consisting of summary statistics of the variables in the data set, creating balance tables and analyzing graphical relationships. For part 2 of the homework, we will analyze the experiment by computing the difference-in-means estimate and its variance. Moreover, we will summarize our findings and take stock by writing a policy memo based on our findings (and possibly extend our findings).\n",
    "\n",
    "**Why are we doing this homework?**\n",
    "\n",
    "1. Lalonde’s analysis of the NSW program demonstrated the power of RCTs over traditional econometric methods using non-experimental data and is one of the reasons that current practice emphasizes the importance of RCTs and approximations of them.\n",
    "2. The data management skills and structure of the data analysis we'll discuss here will lay the foundation for the rest of the quarter.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the data\n",
    "\n",
    "As mentioned above, we'll give you data from the original RCT conducted for the NSW program in the US.\n",
    "\n",
    "### The National Supported Work (NSW) demonstration program.\n",
    "\n",
    "**What did the program do?** The National Supported Work (NSW) demonstration program was a temporary employment program designed to help disadvantaged workers giving them work experience and shelter. This program was run in the US in the late 1970s.  The program provided trainees with work in a sheltered training environment and then assisted them in finding regular jobs.  In more detail, the treatment consisted of a guaranteed job for 9 to 18 months, and the ability to meet with a program counselor regularly to discuss grievances and performance.  An example job would be working at a gas station or a printing shop; the program paid employers to subsidize the employment of the participants.\n",
    "\n",
    "**Who was eligible for the program?** Our dataset includes three target groups selected for inclusion into the program.  (There were more groups targeted by the program, we focus on these three for simplicity): (1) former drug addicts; (2) ex-offenders (that is, people who were convicted of a crime); and (3) young school dropouts. The point was to identify individuals with strong barriers to finding a job.  Note that everyone in our dataset identifies as male.\n",
    "\n",
    "The main criteria were (1) the person must have been currently unemployed (defined as having worked no more than 40 hours in the 4 weeks preceding the time of selection into the program), and (2) the person must have spent no more than 3 months on one regular job of at least 20 hours per week during the preceding 6 months. The program was voluntary, and participants constituted a tiny fraction of the eligible population, implying substantial scope for nonrandom selection into the program.\n",
    " \n",
    "### The RCT data\n",
    "\n",
    "The US government did a randomized control trial as part of NSW.  This data includes a treatment group and a control group, both of whom come from the three groups mentioned above, targeted by the NSW program.\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "You don't have to look at these, but if you are curious, here are some references. \n",
    "\n",
    "* Robert J. LaLonde.  \"Evaluating the Econometric Evaluations of Training Programs with Experimental Data.\"  American Economic Review, September 1986, 76(4): 604-620\n",
    "\n",
    "* Rajeev Dehejia and Sadek Wahba.  \"Causal Effects in Non-Experimental Studies: Reevaluating the Evaluation of Training Programs.\" Journal of the American Statistical Association, 1999, 94(448): 1053-1062.\n",
    "\n",
    "* Rajeev Dehejia and Sadek Wahba.  \"Propensity Score Matching Methods for Non-Experimental Causal Studies.\" Review of Economics and Statistics, 2002, 84: 151-161.\n",
    "\n",
    "* Sebastian Calonico and Jeffrey Smith. \"The Women of the National Supported Work Demonstration.”  Journal of Labor Economics, 2017, 35(S1): S65-S97."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand the overview\n",
    "\n",
    "1. In your own words, summarize why RCTs can help establish causality in the context of the job training experiment.\n",
    "2. What are some possible confounder variables that would make the comparison of participants and non-participants **not** a causal effect? \n",
    "3. Use the potential outcome notation introduced in the lecture to write down the variables of interest in this experiment: what are the potential outcomes, the outcome, what is the treatment, etc. in this case?\n",
    "4. Describe how the fundamental problem of causal inference applies to this experiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>TO DO</font>\n",
    "\n",
    "<font color='red'>Double-click on the box below and fill in your answers to 1-4 above.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**:\n",
    "1. TODO\n",
    "2. TODO\n",
    "3. TODO\n",
    "4. TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Exploratory Analysis\n",
    "\n",
    "**Due:** October 6, 2025, 11:59 pm\n",
    "\n",
    "In this section, we are focusing on familiarizing ourselves with the data set and doing some exploratory analysis. We will guide you through this step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1a: Loading the Data\n",
    "\n",
    "Before you run the cell, make sure you saved the data in the same dictionary as your Jupyter Notebook. \n",
    "\n",
    "Run the following cell (click Shift+Enter) to load the data. Recall from the Python tutorial that we are turning the data into a pandas dataframe to ease computations and visualizations later down the road. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all of our favorite packages...\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Load the csv file\n",
    "datafile = open('jobtraining_rct.csv', newline='')\n",
    "dataDict = csv.DictReader(datafile, delimiter=\",\")\n",
    "\n",
    "# Turn this into a pandas dataframe, and cast the values as numeric\n",
    "expSample = pd.DataFrame(dataDict)\n",
    "for var in dataDict.fieldnames:\n",
    "    expSample[var] = pd.to_numeric(expSample[var], errors='coerce')\n",
    "\n",
    "\n",
    "print(\"Great! Data is loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note:_ Making use of printing statements is a great way to double check that the computer did everything you asked it to do or to de-bug your code if you have more lines (i.e. you can check at eactly which point things are starting to go wrong)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just loaded the data set with a lot of records for different people. When we ask Python to print out the data set, we can see a preview of a few rows as well as all the variables (=columns) the data set contains.\n",
    "\n",
    "Run the following cell to see for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expSample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the bottom, you can see that our data set has 722 rows (_read_ individuals) and 10 columns (_read_ characteristics) stored. We refer to this as the size of our data matrix: 722 x 10.\n",
    "\n",
    "**Here's what each of these variables mean:**\n",
    "* treat: 1 if the subject was in the treatment group of the RCT, 0 otherwise\n",
    "* control: 1 if the subject was in the control group of the RCT, 0 otherwise\n",
    "* age: subject's age in 1975\n",
    "* education: years of education (as of 1975)\n",
    "* Black: 1 if the subject is Black, 0 otherwise\n",
    "* Hispanic: 1 if the subject is Hispanic, 0 otherwise\n",
    "* married: 1 if the subject was married in 1975\n",
    "* nodegree: 1 if the subject did *not* receive a high school degree (as of 1975) \n",
    "* earnings_1975: earnings (in dollars) in 1975 (Adjusted for inflation to 1982 dollars)\n",
    "* earnings_1978: earnings (in dollars) in 1978 (Adjusted for inflation to 1982 dollars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1b: Summary Statistics\n",
    "\n",
    "Applied researchers will typically start off with doing some exploratoy data analysis when they get access to a new data set. This is done to get a deep understanding of the data before randomly trying out different ways of analyzing it. While we know we are dealing with experimental data and usually there is a straightforward way of analyzing such, it is still useful to get an idea of what the data looks like.\n",
    "\n",
    "Doing an exploratory data analysis may include a lot of different things, e.g. computing averages and standard deviations of specific variables, plotting histograms, creating scatter plots of the outcome of interest and other variables that we might expect to have a relationship with the outcome variable. Any of these may inform choices about variable transformations pre-analysis or what variables we might want to include for sure in our analysis.\n",
    "\n",
    "To get started, let's look at an arbitrary person in the dataset, and then compute some summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print out an arbitary person:\n",
    "arbPerson = expSample.iloc[10] # this accesses the record for the individual in row 11\n",
    "print(\"Here's someone!\")\n",
    "print(arbPerson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to some summary statistics. We first focus on earnings in 1975 and 1978."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first need to import the numpy package. Recall that we use the second half of the statement\n",
    "# (as np) to tell python that we will abbreviate the numpy package as np going forward. This \n",
    "# is why we will later on write np.mean() instead of numpy.mean(). It is simply to ease notation for \n",
    "# us and readability.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## Compute the average earnings in 1975 and 1978.\n",
    "##\n",
    "## NOTE: You could of course also program the mean function up yourself by using the .sum() function\n",
    "## and dividing by the sample size.\n",
    "\n",
    "e75 = expSample[\"earnings_1975\"] # Access only the column called \"earnings_1975\" in the data set\n",
    "e78 = expSample[\"earnings_1978\"]\n",
    "\n",
    "print(\"Avg earnings in 1975:\", np.mean(e75))\n",
    "print(\"Avg earnings in 1978:\", np.mean(e78))\n",
    "\n",
    "## We can also print out the STANDARD DEVIATION of earnings in 1975 and 1978\n",
    "print(\"std. of earnings in 1975:\", np.std(e75))\n",
    "print(\"std. of earnings in 1978:\", np.std(e78))\n",
    "\n",
    "## Plot the histogram of earnings in 1975 and 1978:\n",
    "from matplotlib import pyplot as plt # Import the plot package\n",
    "print(\"\")\n",
    "print(\"Histogram of Earnings in 1975 and 1978\")\n",
    "plt.hist([e75, e78], label=[\"Earnings 1975\", \"Earnings 1978\"])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, it is good to have an overview of **all** variables contained in the data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Coding time!</font>\n",
    "\n",
    "<font color=\"red\"> Pick two other covariates (not treat or control) in the data set and provide the same summary statistics (mean, standard deviation and histograms) as above for them. Feel free to add any other summary statistic you might find important. You can check the \"WelcomeToPython\" tutorial for how to code quantities like the minimum, maximum, medians or other percentiles in Python.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**New Python Trick!** Instead of having to copy and paste the above box a lot of times, Python has nice functions for us that do this automatically (everything minus creating histograms).\n",
    "Below, we are using the .describe() function for Pandas dataframes to calculate summary statistics for all variables contained in the data set as once. It returns the number of observations (_count_), the mean for each variable (_mean_), the standard deviation (_std_), the minimum value (_min_), the 25th pecentile (_25%_), the 50th pecentile (_50%_), the 75th pecentile (_75%_) and the maximum value (_max_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expSample.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Interpretation time!</font>\n",
    "\n",
    "<font color=\"red\"> Do you find anything surprising in the data from the summary statistics? If yes, what is it and why is it surprising? If not, why not?\n",
    "    \n",
    "<font color=\"red\">**TODO:** Double-click the box below and enter your answer. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1c: Balance table\n",
    "\n",
    "In addition to knowing which individual received treatment, the data set we have contains a lot of other characteristics about each individual. Hence, before diving into the experimental analysis, it is common practice to compute a so-called balance table. A balance table compares how the covariates of the treatment and control group are distributed. This comparison is often done by comparing the means in the treatment group and control group, but sometimes we want to compare the entire distribution too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Interpretation time!</font>\n",
    "\n",
    "<font color=\"red\"> Before computing anything, what do you expect the balance of the the treatment and control group to look like for the different covariates?\n",
    "    \n",
    "<font color=\"red\">**TODO:** Double-click the box below and enter your answer. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first focus on earnings in 1975 again. If you need a refresher on list comprehensions in Python, see the \"WelcomeToPython\" tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earnings in 1975\n",
    "\n",
    "# Define the number of individuals in the sample\n",
    "num_indiv = expSample.shape[0] # number of individuals in the sample\n",
    "\n",
    "# Separate out all the treated individuals\n",
    "earn_1975_t = [expSample.loc[x,\"earnings_1975\"] for x in range(num_indiv) if expSample.loc[x,\"treat\"] ]\n",
    "\n",
    "# Separate out all the not treated (= control) individuals\n",
    "earn_1975_c = [expSample.loc[x,\"earnings_1975\"] for x in range(num_indiv) if not expSample.loc[x,\"treat\"] ]\n",
    "\n",
    "# Calculate their means and compare them\n",
    "print(\"Avg. treat earnings in 1975:\", np.mean(earn_1975_t))\n",
    "print(\"Avg. control earnings in 1975:\", np.mean(earn_1975_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Coding time!</font>\n",
    "\n",
    "<font color=\"red\"> Use the same two other covariates in the data set you picked in part 1 of the assignment and compare their means.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:** If you look at a variable that takes on more than just 0/1 values (like 1975 earnings, or age, or years of education), find some way of looking at the whole *distribution* across the treatment and the control group (e.g., plot the histogram of the variable for the treated individuals and the control individuals).  Do they look about the same?  \n",
    "\n",
    "**TIP/New Python Trick!** If you plot multiple histograms on the same chart, like we did above, you may want to add the flag \"density=True\" in the arguments of plt.hist.  For example:\n",
    "```\n",
    "plt.hist([myList1, myList2], label=[\"thing1\", \"thing2\"], density=True)\n",
    "\n",
    "```\n",
    "That will normalize the histograms so that you can compare them even if there are a different number of observations in myList1 and myList2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTIONAL TODO: Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Interpretation time!</font>\n",
    "\n",
    "<font color=\"red\"> After computing, what is your take-away? Do you think that we can trust the results from the RCT analysis we will do in part 2c?\n",
    "\n",
    "<font color=\"red\">**TODO:** Double-click on the box below and enter your answer. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1d: Graphical relationships\n",
    "\n",
    "Now, we will start with our analysis to answer our relationship of interest: \"Did the job training program increase earnings?\". We will first document whether the two variables of interest are correlated by creating a scatterplot and calculating the correlation coefficient.\n",
    "\n",
    "Run the cell below to create the scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the plotting package we need\n",
    "# Note: this is unnecessary if you already loaded it for the histograms in part 1\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# You don't actually HAVE TO define e78 again if you ran the cell with the summary statistics already\n",
    "treat = expSample[\"treat\"] \n",
    "e78 = expSample[\"earnings_1978\"]\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.scatter(np.array(treat),np.array(e78))\n",
    "plt.xlabel(\"Treatment\") # This is how we can change the label of the x-axis\n",
    "plt.ylabel(\"1978 Earnings\") # This is how we can change the label of the y-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Interpretation time!</font>\n",
    "\n",
    "<font color=\"red\"> Before computing anything else, does this correlation look positive, negative or non-existent to you? Does the sign of the correlation intuitively make sense to you?\n",
    "    \n",
    "<font color=\"red\">**TODO:** Double-click the box below and enter your answer. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, it can be hard to judge simply from a plot what the sign of the correlation is. Luckily, we learned a formula in the lectures on how to calculate it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Coding time!</font>\n",
    "\n",
    "<font color=\"red\"> It is your turn to write code for the correlation coefficient. (We saw the formula for correlation in Lecture 2).\n",
    "\n",
    "<font color=\"red\"> **Hint:** Think about which components you need. Then, look at the previous examples in part 1 to figure out how to code up the different components of the correlation coefficient formula. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Interpretation time!</font>\n",
    "\n",
    "<font color=\"red\"> How can we interpret the correlation coefficient we obtained between treatment and outcome?\n",
    "\n",
    "<font color=\"red\">**TODO:** Double-click the box below and enter your answer. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">!! STOP !!</font>\n",
    "\n",
    "<font color=\"blue\"> You have finished part 1 of the homework. This is the only part of the homework that is due on October 6, 2025, 11:59 pm. Save your progress and please upload your Jupyter notebook on Gradescope.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Analysis\n",
    "\n",
    "**Due:** October 13, 2025, 11:59 pm\n",
    "\n",
    "We will split our analysis into two parts: a) causal analysis (including treatment effect estimation and inference) and b) writing a policy memo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2a: Causal Analysis\n",
    "\n",
    "We will move to our causal analysis. We learned in class that there is an important distinction between correlations and causal statements. To obtain **causal estimates** of whether (and by how much) job training programs increase future earnings, we will compute the **average treatment effect** of:\n",
    "\n",
    "1. The NSW program on earnings in 1978, as measured by the RCT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the simple difference in means estimator that we first encountered in the lecture to get an estimate of the average treatment effect.\n",
    "\n",
    "Remember, you want to use individuals with the \"treat\" variable set to \"1\" for the treatment; and individuals with the \"control\" variable set to \"1\" for the control.\n",
    "\n",
    "**Tip 1:** Try writing down first what it is you want to compute. \n",
    "\n",
    "**Tip 2:** Recall from part 2a how we can subset data in Python using list comprehensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Coding time!</font>\n",
    "\n",
    "<font color=\"red\">Write code below to compute the ATE from above, according to the RCT, and print out a statement like:  </font> \n",
    "\n",
    "1. <font color=\"red\"> \"The ATE of NSW on earnings in 1978 is: _____\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Interpretation time!</font>\n",
    "\n",
    "1. <font color=\"red\">How does the treatment effect relate to the correlation coefficient we calculated in the previous assignment, what is the same, what is different? </font> \n",
    "2. <font color=\"red\">Are there any reasons you can think of that might threaten the causal interpretation of our treatment effect estimate? </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will calculate the **variance of the estimator**. In the lecture, we saw two different ways of getting an estimate for the variance: i) analytical variance and ii) bootstrap variance. We will focus on the bootstrap variance in this problem set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Coding time!</font>\n",
    "\n",
    "<font color=\"red\"> Add in the code below your way of estimating the average treatment effect in each bootstrap sample using the difference-in-means estimator. You can reuse code from earlier. </font> \n",
    "\n",
    "<font color=\"red\">**NOTE:** This code may take a few minutes to run. </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap variance\n",
    "rng = np.random.default_rng(seed=10)  # create a Generator\n",
    "\n",
    "B = 10000 # number of bootstrap samples\n",
    "num_indiv = expSample.shape[0] # number of individuals in the sample\n",
    "num_t = np.sum(expSample[\"treat\"]) # number of treated individuals in the sample\n",
    "num_c = np.sum(expSample[\"control\"]) # number of control individuals in the sample\n",
    "tau_hat_star_list = [] # define an empty list where our estimates of each bootstrap sample can go\n",
    "\n",
    "# for loop for bootstrap\n",
    "for b in range(B):\n",
    "\n",
    "    # Randomly pick num_t (size) numbers from a list of indeces for the  \n",
    "    # treated sample (idx_t) with replacement (replace = true)\n",
    "    idx_t = expSample[expSample[\"treat\"]==1].index.tolist() # gives us a list of all indices of treated units\n",
    "    bidx_t  = rng.choice(idx_t,replace=True,size=num_t) \n",
    "    idx_c = expSample[expSample[\"control\"]==1].index.tolist() # gives us a list of all indices of control units\n",
    "    bidx_c  = rng.choice(idx_c,replace=True,size=num_c)\n",
    "\n",
    "    bidx_all = np.concatenate((bidx_t,bidx_c)) # concatenate the two list of indices into one\n",
    "\n",
    "    # Create a bootstrap sample with those units\n",
    "    b_samp = expSample.iloc[bidx_all,:]\n",
    "\n",
    "    b_samp.loc[:,\"treat\"] = b_samp.loc[:,\"treat\"].astype(int)\n",
    "    b_samp.loc[:,\"control\"] = b_samp.loc[:,\"control\"].astype(int)\n",
    "\n",
    "    b_samp = b_samp.reset_index() # reset the index to start from 0\n",
    "\n",
    "    #########################################################\n",
    "    \n",
    "    # Calculate the difference-in-means estimate for the bootstrap sample\n",
    "    # Please call it tau_hat_star = ...\n",
    "\n",
    "    #TODO: Your code here.\n",
    "\n",
    "    #########################################################\n",
    "\n",
    "    # Append the estimate to our list\n",
    "    tau_hat_star_list.append(tau_hat_star)\n",
    "\n",
    "\n",
    "# Calculate the variance using the bootstrap estimates\n",
    "var = np.std(tau_hat_star_list)**2 # sample variance of the list of observed bootstrap estimates, tau_hat_star\n",
    "\n",
    "print(\"The bootstrap variance of our Difference-in-Means estimator is:\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, the reason why we will estimate the variance of an estimator is because we are interested in **inference**. In this homework, we will focus on **hypothesis testing** for conducting inference: \"Is our average treatment effect different from zero?\". As you saw in class, the general procedure after we have estimated the variance is to pretend that the difference-in-means estimator is normally distributed with a mean of the true treatment effect and variance equal to our estimate. We then use this assumption to test for null effects.\n",
    "\n",
    "**Tip 1:** The procedure for testing is outlined in detail in lecture 2.  \n",
    "\n",
    "**Tip 2:** Recall that testing at 5\\% significance means that we are using a cut-off value from the normal distribution that is equal to 1.96. \n",
    "\n",
    "**Tip 3:** Recall from the \"WelcomeToPython\" tutorial that we can use **if** statements as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 5\n",
    "if s > 10:\n",
    "    print(\"The number is bigger than 10.\")\n",
    "else:\n",
    "    print(\"The number is not bigger than 10.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Coding time!</font>\n",
    "\n",
    "<font color=\"red\"> Test whether the job training program had any effect on future earnings at a 5\\% significance level. You should print out a statement like:</font>\n",
    "\n",
    "1. <font color=\"red\">  \"We estimate a p-value of less than 0.05, hence we reject the null that tau = 0 at a 5\\% significance level. </font>\n",
    "2. <font color=\"red\">  \"We estimate a p-value larger than 0.05, hence we do not reject the null that tau = 0 at a 5\\% significance level. </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2b: Policy Memo\n",
    "\n",
    "Imagine you have been hired as a policy analyst for the State Department of Labor, which is deciding whether to invest in a new job training initiative modeled after the program you analyzed in this week’s homework.\n",
    "\n",
    "Draft a policy memo that summarizes what the evidence shows and makes a clear, actionable recommendation for the agency’s leadership team. The commissioner and senior staff do not have the same technical background as you, so your memo should explain the findings and their implications in non-technical language.\n",
    "\n",
    "Your memo should be rooted in your empirical findings and critical evaluation from earlier sections, but you are encouraged to conduct additional analyses as well or draw on other studies seen in class:\n",
    "\n",
    "* **State your recommendation clearly.** Should the state adopt a similar program?*\n",
    "*  **Summarize the key evidence.** Highlight the most policy-relevant results (e.g., estimated treatment effects, subgroup differences, cost per participant) in clear, non-technical terms. Here, you should include findings from earlier sections. You can also include results from any additional analyses you have conducted or discussion around findings from studies seen in class.\n",
    "*  **Discuss policy design.** If recommending adoption, please explain how you would design or adapt the program for your state. If discouraging adoption, suggest alternative strategies the state might consider.\n",
    "*  **Confront limitations.**  Use the framework from class (external validity, construct validity, spillover effects, ethical concerns) to honestly discuss what the evidence does and does not tell us.\n",
    "*  **Propose next steps.** If you had more time, money, or data, what further research or pilot testing would most help the agency make a confident decision? \n",
    "\n",
    "_Deliverables:_\n",
    "* A policy memo (max. 1,000-word), written for the Commissioner of Labor and her senior staff, with 1-2 small exhibits maximum (a table or figure).\n",
    "* A technical appendix (1-2 page) with supporting tables and figures for readers who want more details.\n",
    "\n",
    "_Suggested memo structure:_\n",
    "1) Policy recommendation (2-3 sentences)\n",
    "2) Evidence summary\n",
    "3) Policy design/implementation plan\n",
    "4) Cost-effectiveness (if applicable)\n",
    "5) Limitation of the evidence\n",
    "6) Next steps for research\n",
    "\n",
    "\n",
    "*It is perfectly acceptable to recommend not adopting the program. If you take this position, walk the policymakers through your reasoning and propose a clear plan for gathering better evidence. You can still use the next points and the suggested memo structure as guidance for your policy memo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">!! DON'T FORGET !!</font>\n",
    "\n",
    "<font color=\"blue\"> Hand in your policy memo in a pdf format on Gradescope as well as your completed Jupyter notebook with your team's names on it, e.g. \"Anna_Amar_Mary_Guido_Assignment_1.ipynb\" and \"Anna_Amar_Mary_Guido_Policy_Memo_1.pdf\".</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##OPTIONAL TODO (policy memo): Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up and take-aways\n",
    "\n",
    "Congrats on finishing the first homework! By the end of this homework, you should hopefully feel a bit more familiar with Python. We walked step-by-step through the general structure that will guide a lot of our data analyses going forward: 1) exploratory analysis: familiarizing ourselves with the data by calculating summary statistics and 2) analysis: correlations and graphical relations and causal analysis (estimation and inference). Moreover, an important step that researchers conduct between their exploratory analysis and analysis is creating a balance table to check whether the randomization of the experiment was successful and we can trust our RCT estimates.\n",
    "\n",
    "After our experimental unit, you should be able to answer the following questions:\n",
    "* What is an RCT and why is it powerful?\n",
    "* How can we estimate the average treatment effect of an RCT? Is this estimator unbiased?\n",
    "* How can we calculate its variance?\n",
    "* How can we conduct inference on the average treatment effect?\n",
    "* What are some limitations of RCTs?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
